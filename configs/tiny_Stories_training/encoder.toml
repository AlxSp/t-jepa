block_size = 1024
vocab_size = 32000 # LLAMA tokenizer is 32000 which is a multiple of 64 for efficiency
n_layer = 8
n_head = 12
n_embd = 384
rotary_n_embd = 32
dropout = 0.0
bias = true # True: bias in Linears and LayerNorms like GPT-2. False: a bit better on small datasets